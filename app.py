import streamlit as st
from PIL import Image
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
import os
import requests

# ---------------------------
# Streamlit Page Setup
# ---------------------------
st.set_page_config(
    page_title="üñºÔ∏è Image Captioning App",
    page_icon="üñºÔ∏è",
    layout="centered"
)

st.title("üñºÔ∏è Image Captioning App with BLIP-2 + Optional LLM Enhancement")
st.markdown("""
Upload an image and get an **accurate caption**.  
Optionally, enhance the caption using `llama-3.3-70b-versatile` via Groq API for more vivid descriptions.
""")

# ---------------------------
# Load BLIP-2 Model
# ---------------------------
@st.cache_resource(show_spinner=True)
def load_blip_model():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

processor, blip_model = load_blip_model()

# ---------------------------
# GROQ API Key
# ---------------------------
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    st.warning("‚ö†Ô∏è GROQ_API_KEY not set. LLM enhancement will be disabled.")

# ---------------------------
# File Upload
# ---------------------------
uploaded_image = st.file_uploader("Upload an image", type=["png", "jpg", "jpeg"])
enhance_with_llm = st.checkbox("Enhance caption with llama-3.3-70b-versatile", value=False)

# ---------------------------
# BLIP-2 Caption Function
# ---------------------------
def generate_blip_caption(image):
    inputs = processor(images=image, return_tensors="pt")
    out = blip_model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption

# ---------------------------
# LLM Enhancement Function
# ---------------------------
def enhance_caption_llm(caption_text):
    if not GROQ_API_KEY:
        return "‚ö†Ô∏è Cannot enhance: GROQ_API_KEY not set."
    
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }

    prompt = f"""
You are an expert AI assistant. Refine and enhance the following image caption to be more vivid, detailed, and descriptive.  
Keep the original meaning but enrich objects, scene, colors, and mood.

Caption:
\"\"\"{caption_text}\"\"\"
"""

    payload = {
        "model": "llama-3.3-70b-versatile",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7
    }

    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        try:
            result = response.json()
            return result['choices'][0]['message']['content']
        except:
            return "‚ö†Ô∏è Error parsing Groq response."
    else:
        return f"‚ö†Ô∏è Groq API Error: {response.status_code} - {response.text}"

# ---------------------------
# Main Processing
# ---------------------------
if uploaded_image:
    image = Image.open(uploaded_image).convert("RGB")
    st.image(image, caption="Uploaded Image", use_column_width=True)

    if st.button("Generate Caption"):
        with st.spinner("Generating BLIP-2 caption..."):
            blip_caption = generate_blip_caption(image)
        st.subheader("BLIP-2 Caption")
        st.write(f"üìù {blip_caption}")

        if enhance_with_llm:
            with st.spinner("Enhancing caption with llama-3.3-70b-versatile..."):
                enhanced_caption = enhance_caption_llm(blip_caption)
            st.subheader("Enhanced Caption")
            st.write(f"‚ú® {enhanced_caption}")
